<div class="research-section">
    <h2>Research Projects</h2>
    <div class="project">
        <img src="images\tnr-temp.png" alt="Project 1" class="project-img">
        <h3>Tag and Release</h3>
        <p>Benchmark datasets are critical to the evolution of AI efforts yet often embed unintended biases that influence the models that drive human-AI interactions. A deeper inspection and awareness of data is needed to understand the biases datasets may contain. We introduce the Tag-and-Release method, inspired from wildlife research, that treats data as an organism and examines how different environments (i.e., CNNs) select for unique traits or characteristics that ultimately impact data's survival. Using the canonical MNIST handwritten digit dataset as a case study, we describe how the Tag-and-Release method can be used to analyze how dataset imbalance biases propagate into different neural architectures. We demonstrate how the technique can be scaled to coordinate data inspection efforts with crowd workers to annotate the MNIST dataset. Using tagged data, we conducted a user study with 11 machine learning students to evaluate whether our method enhances understanding of ML explainability. We present our findings for developing balanced and fair datasets, stimulate discussions about models as ecosystems, and advocate for a data conservatory for coordinated efforts to support explainable AI initiatives within intelligent systems.</p>
        <h4><i class="fas fa-cogs"></i> Keywords:</h4>
        <ul class="keywords">
            <li>Explainable AI</li>
            <li>Model Analysis</li>
            <li>Data Labeling</li>
            <li>Crowdsourcing</li>
        </ul>
        <h4><i class="fas fa-microchip"></i> Components Involved:</h4>
        <ul class="comp-list">
            <li>Labeling UI</li>
            <li>ML Modeling</li>
            <li>Mechanical Turk</li>
            <li>User Study</li>
            <li>Qualitative Analysis</li>
        </ul>
        <a href="project-1-more-info.html" class="btn">Learn More</a>
    </div>
    <div class="project">
        <img src="images\braidflow.png" alt="Project 2" class="project-img">
        <h3>BraidFlow</h3>
        <p>Entering a cognitive state of flow is a natural response of the mind that allows people to fully concentrate and cope with tedious, and often repetitive tasks. Understanding how to trigger or sustain flow remains limited by retrospective surveys, presenting a need to better document flow. Through a validation study, we first establish braidmaking as a flow-inducing task. We then study how braidmaking can be used to unpack the experience of flow on a moment-by-moment basis. Using an instrumented Kumihimo braidmaking tool and off-the-shelf biosignal wristbands, we record the experiences of 24 users engaged in 3 different braidmaking tasks. Feature vectors motivated from flow literature were extracted from activity data (IMU, EMG, EDA, heart rate, skin temperature, braiding telemetry) and annotated with Flow Short Scale (FSS) scores. Together, this dataset and data-capture system form the first open-access and holistic platform for mining flow data and synthesizing flow-aware design principles.</p>
        <h4><i class="fas fa-cogs"></i> Keywords:</h4>
        <ul class="keywords">
            <li>Dataset</li>
            <li>Sensor Data</li>
            <li>Creativity</li>
            <li>Cognitive Flow</li>
        </ul>
        <h4><i class="fas fa-microchip"></i> Components Involved:</h4>
        <ul class="comp-list">
            <li>Digital Fabrication</li>
            <li>User Study</li>
            <li>MongoDB</li>
            <li>Quantitative Analysis</li>
            <li>Qualitative Analysis</li>
        </ul>
        <a href="project-2-more-info.html" class="btn">Learn More</a>
    </div>
    <div class="project">
        <img src="images\duckcheck.png" alt="Project 3" class="project-img">
        <h3>DuckCheck</h3>
        <p>Debugging is a programming practice critical to developing functional and maintainable software. While integrated development environments (IDEs) provide powerful tools to support program correctness, debugging remains a tacit, solitary, and invisible practice that places an emotional toil on novice programmers. To understand the space of embodied, connected, and tangible debugging, we examine how the programming environment can be extended to include the programmer's physical space and body. We introduce a hybrid debugging environment (HDE) called DuckCheck that leverages an interactive avatar, computational notebook plugin, and biosignal wristband to create playful, ambient, and informative interactions that alter the debugging experience. We demonstrate how the system can collect insightful telemetry to ground perceptions of debugging behaviors. In a user study with novice programmers, we evaluate how debugging support interactions alter programmer behaviors, and report observed themes for informing the design of debugging support tools.</p>
        <h4><i class="fas fa-cogs"></i> Keywords:</h4>
        <ul class="keywords">
            <li>Programming</li>
            <li>Interactive Development Environment</li>
            <li>Ambient Devices</li>
            <li>Digital Fabrication</li>
        </ul>
        <h4><i class="fas fa-microchip"></i> Components Involved:</h4>
        <ul class="comp-list">
            <li>Jupyter Plugin</li>
            <li>Tangible Interface</li>
            <li>Tacit Debugging Practices</li>
        </ul>
        <a href="project-3-more-info.html" class="btn">Learn More</a>
    </div>
    <div class="project">
        <img src="images\hrp.png" alt="Project 4" class="project-img">
        <h3>Hum, Rattle and Purr</h3>
        <p>From the subtle scritch of a fingernail to the voracious buzz of a table saw, creative spaces are filled with a medley of sounds. These sounds inherently encode valuable information that over time is internalized by practitioners as feedback mechanisms to confirm appropriate use, adjust behaviors, guide creativity, or diagnose malfunction. As an unobstructive data source, audio data holds great promise in serving as the foundation of smart interactions in workshop environments; however, such data-driven systems are difficult to realize in real-world contexts where data is not cleanly segmented, labeled, or available. In this paper, we describe the design of an ad hoc audio-logging system for building and refining models of activity within a glass coldworking studio. We demonstrate how foregrounding data collection can produce robust and repairable deep learning models and support the creation of bespoke smart tool interactions.</p>
        <h4><i class="fas fa-cogs"></i> Keywords:</h4>
        <ul class="keywords">
            <li>Smart Tools</li>
            <li>Smart Environments</li>
            <li>Data Collection</li>
            <li>Audio Data</li>
        </ul>
        <h4><i class="fas fa-microchip"></i> Components Involved:</h4>
        <ul class="comp-list">
            <li>IoT Audio</li>
            <li>Realtime Sonic Classifier</li>
            <li>Data Labeling</li>
            <li>ML Modeling</li>
            <li>Smart Workshop Interactions</li>
            <li>Glass Grinding</li>
        </ul>
        <a href="project-4-more-info.html" class="btn">Learn More</a>
    </div>
    <div class="project">
        <img src="images\ladderbot.png" alt="Project 5" class="project-img">
        <h3>Ladderbot</h3>
        <p>Interviewing is a critical skill for understanding user needs and values. While a breadth of interviewing methods provides structure to the process, the interview process can quickly overwhelm learners due to its multi-cognitive approach, requiring a learner to simultaneously listen, reflect, interpret, and react. Virtual users, or AI agents representing users, show promise in supporting learners with an always-available practice user. Yet, less is known about how human-AI interactions affect the learning experience. This work describes a laddering interview workshop study where participants practiced their interviewing skills with each other and a virtual user across two mediums: via voice/video call and through a texting application. Using a Wizard of Oz method, the virtual user was played by wizards trained to emulate a predefined persona. Applying thematic analysis across 40 transcripts, we describe themes in conversational flow, social cues, and the learning experience and discuss their implications for interview training.</p>
        <h4><i class="fas fa-cogs"></i> Keywords:</h4>
        <ul class="keywords">
            <li>UI/UX Design</li>
            <li>Chatbot</li>
            <li>Laddering Interview</li>
            <li>Wizard of Oz</li>
        </ul>
        <h4><i class="fas fa-microchip"></i> Components Involved:</h4>
        <ul class="comp-list">
            <li>Workshop</li>
            <li>User Study</li>
            <li>Qualitative Analysis</li>
        </ul>
        <a href="project-5-more-info.html" class="btn">Learn More</a>
    </div>
</div>
